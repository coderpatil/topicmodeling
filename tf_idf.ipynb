{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "d:\\binghamton\\semester1\\IS2 502\\topicmodeling\\env\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import warnings, re\n",
    "warnings.simplefilter('ignore')\n",
    "from itertools import chain\n",
    "\n",
    "import gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "from gensim.models.coherencemodel import CoherenceModel \n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_a</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoritecount</th>\n",
       "      <th>replytosn</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replytosid</th>\n",
       "      <th>id</th>\n",
       "      <th>replytouid</th>\n",
       "      <th>statussource</th>\n",
       "      <th>screenname</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>isretweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>American Harem.. #MeToo https://t.co/HjExLJdGuF</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-29T23:59:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.360000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://instagram.com\" rel=\"nofollow\"&gt;...</td>\n",
       "      <td>ahmediaTV</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@johnconyersjr  @alfranken  why have you guys ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>johnconyersjr</td>\n",
       "      <td>2017-11-29T23:59:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.360000e+17</td>\n",
       "      <td>266149840</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>JesusPrepper74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Watched Megan Kelly ask Joe Keery this A.M. if...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-29T23:59:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.360000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>DemerisePotvin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Women have been talking about this crap the en...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-29T23:59:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.360000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>TheDawnStott</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>.@BetteMidler please speak to this sexual assa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-11-29T23:59:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.360000e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>scottygirl2014</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  column_a                                               text  favorited  \\\n",
       "0        1    American Harem.. #MeToo https://t.co/HjExLJdGuF        0.0   \n",
       "1        2  @johnconyersjr  @alfranken  why have you guys ...        0.0   \n",
       "2        3  Watched Megan Kelly ask Joe Keery this A.M. if...        0.0   \n",
       "3        4  Women have been talking about this crap the en...        0.0   \n",
       "4        5  .@BetteMidler please speak to this sexual assa...        0.0   \n",
       "\n",
       "  favoritecount      replytosn              created  truncated  replytosid  \\\n",
       "0             0            NaN  2017-11-29T23:59:00        0.0         NaN   \n",
       "1             0  johnconyersjr  2017-11-29T23:59:00        0.0         NaN   \n",
       "2             0            NaN  2017-11-29T23:59:00        1.0         NaN   \n",
       "3             0            NaN  2017-11-29T23:59:00        0.0         NaN   \n",
       "4            15            NaN  2017-11-29T23:59:00        0.0         NaN   \n",
       "\n",
       "             id replytouid                                       statussource  \\\n",
       "0  9.360000e+17        NaN  <a href=\"http://instagram.com\" rel=\"nofollow\">...   \n",
       "1  9.360000e+17  266149840  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   \n",
       "2  9.360000e+17        NaN  <a href=\"http://twitter.com/download/android\" ...   \n",
       "3  9.360000e+17        NaN  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   \n",
       "4  9.360000e+17        NaN  <a href=\"http://twitter.com/#!/download/ipad\" ...   \n",
       "\n",
       "       screenname  retweetcount  isretweet  retweeted  longitude  latitude  \\\n",
       "0       ahmediaTV           0.0        0.0        0.0        NaN       NaN   \n",
       "1  JesusPrepper74           0.0        0.0        0.0        NaN       NaN   \n",
       "2  DemerisePotvin           0.0        0.0        0.0        NaN       NaN   \n",
       "3    TheDawnStott           0.0        0.0        0.0        NaN       NaN   \n",
       "4  scottygirl2014          11.0        0.0        0.0        NaN       NaN   \n",
       "\n",
       "  location  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('metoo_tweets_dec2017.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = data.filter(['text'], axis=1)\n",
    "filtered_data.head()\n",
    "filtered_data = filtered_data[filtered_data[\"text\"].apply(lambda x: type(x) == str)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the false and nanvalues\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words  |= set(['ï¿½ï¿½', 'ï¿½', 'x000d', 'rcc', 'mex000d', 'leaguex000daccording', 'jr', 'donohue', 15, 1, 200, 'x000dx000d', 'im', 'u'])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'RT[\\s]+', '', text)\n",
    "    text = re.sub(r'ï¿½', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = strip_non_alphanum(text)\n",
    "    text =  ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word.lower() not in stop_words])\n",
    "    return text\n",
    "\n",
    "filtered_data['clean_text'] = filtered_data['text'].apply(clean_text)\n",
    "filtered_data['clean_tokens'] = filtered_data['clean_text'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American Harem.. #MeToo https://t.co/HjExLJdGuF</td>\n",
       "      <td>american harem</td>\n",
       "      <td>[american, harem]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@johnconyersjr  @alfranken  why have you guys ...</td>\n",
       "      <td>guy resigned yet liberal hypocrisy</td>\n",
       "      <td>[guy, resigned, yet, liberal, hypocrisy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Watched Megan Kelly ask Joe Keery this A.M. if...</td>\n",
       "      <td>watched megan kelly ask joe keery rub finger h...</td>\n",
       "      <td>[watched, megan, kelly, ask, joe, keery, rub, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women have been talking about this crap the en...</td>\n",
       "      <td>woman talking crap entire time finally someone...</td>\n",
       "      <td>[woman, talking, crap, entire, time, finally, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@BetteMidler please speak to this sexual assa...</td>\n",
       "      <td>please speak sexual assault interview</td>\n",
       "      <td>[please, speak, sexual, assault, interview]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0    American Harem.. #MeToo https://t.co/HjExLJdGuF   \n",
       "1  @johnconyersjr  @alfranken  why have you guys ...   \n",
       "2  Watched Megan Kelly ask Joe Keery this A.M. if...   \n",
       "3  Women have been talking about this crap the en...   \n",
       "4  .@BetteMidler please speak to this sexual assa...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                                     american harem   \n",
       "1                 guy resigned yet liberal hypocrisy   \n",
       "2  watched megan kelly ask joe keery rub finger h...   \n",
       "3  woman talking crap entire time finally someone...   \n",
       "4              please speak sexual assault interview   \n",
       "\n",
       "                                        clean_tokens  \n",
       "0                                  [american, harem]  \n",
       "1           [guy, resigned, yet, liberal, hypocrisy]  \n",
       "2  [watched, megan, kelly, ask, joe, keery, rub, ...  \n",
       "3  [woman, talking, crap, entire, time, finally, ...  \n",
       "4        [please, speak, sexual, assault, interview]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\binghamton\\semester1\\IS2 502\\topicmodeling\\env\\lib\\site-packages\\gensim\\models\\lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
      "  sparsetools.csc_matvecs(\n",
      "d:\\binghamton\\semester1\\IS2 502\\topicmodeling\\env\\lib\\site-packages\\gensim\\models\\lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
      "  sparsetools.csc_matvecs(\n",
      "d:\\binghamton\\semester1\\IS2 502\\topicmodeling\\env\\lib\\site-packages\\gensim\\models\\lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
      "  sparsetools.csc_matvecs(\n",
      "d:\\binghamton\\semester1\\IS2 502\\topicmodeling\\env\\lib\\site-packages\\gensim\\models\\lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
      "  sparsetools.csc_matvecs(\n",
      "d:\\binghamton\\semester1\\IS2 502\\topicmodeling\\env\\lib\\site-packages\\gensim\\models\\lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
      "  sparsetools.csc_matvecs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.381*\"sexual\" + 0.322*\"misconduct\" + 0.312*\"milano\" + 0.306*\"allegation\" + 0.305*\"president\" + 0.303*\"list\" + 0.302*\"state\" + 0.300*\"america\" + 0.300*\"full\" + 0.300*\"united\"'), (1, '0.436*\"woman\" + 0.433*\"movement\" + 0.335*\"year\" + 0.334*\"time\" + 0.331*\"person\" + 0.216*\"amp\" + 0.139*\"trump\" + 0.135*\"silence\" + 0.126*\"story\" + 0.107*\"magazine\"'), (2, '0.658*\"woman\" + -0.276*\"movement\" + -0.265*\"time\" + -0.222*\"person\" + -0.218*\"year\" + 0.153*\"amp\" + 0.117*\"sexual\" + -0.098*\"magazine\" + -0.086*\"gave\" + -0.085*\"interview\"'), (3, '0.421*\"farrow\" + 0.421*\"dylan\" + 0.395*\"woody\" + 0.391*\"allen\" + 0.388*\"revolution\" + 0.385*\"spared\" + 0.117*\"via\" + -0.064*\"amp\" + -0.034*\"person\" + -0.034*\"year\"'), (4, '-0.502*\"amp\" + 0.296*\"woman\" + -0.175*\"franken\" + -0.173*\"al\" + -0.170*\"iâ\" + -0.167*\"set\" + -0.160*\"donâ\" + -0.160*\"stone\" + -0.159*\"pâ\" + -0.159*\"leeann\"'), (5, '0.296*\"year\" + 0.283*\"person\" + -0.238*\"like\" + -0.232*\"asked\" + -0.232*\"comment\" + -0.232*\"interview\" + -0.231*\"deemed\" + -0.231*\"value\" + -0.231*\"whyâ\" + -0.231*\"id\"'), (6, '-0.529*\"sexual\" + 0.236*\"milano\" + -0.199*\"harassment\" + -0.193*\"trump\" + -0.173*\"assault\" + 0.162*\"fear\" + -0.159*\"come\" + -0.148*\"accuser\" + 0.143*\"2017\" + -0.141*\"courage\"'), (7, '-0.324*\"silence\" + -0.286*\"fear\" + 0.260*\"movement\" + -0.189*\"experience\" + -0.188*\"wrote\" + -0.187*\"recent\" + -0.187*\"major\" + -0.187*\"paper\" + -0.184*\"attempt\" + -0.184*\"contrived\"'), (8, '-0.342*\"burke\" + -0.282*\"cover\" + -0.256*\"black\" + -0.253*\"u\" + -0.241*\"deserves\" + -0.231*\"dear\" + -0.229*\"feature\" + -0.228*\"solo\" + -0.228*\"elevate\" + -0.228*\"publicationsx000dx000dtarana\"'), (9, '-0.491*\"movement\" + 0.261*\"breaker\" + 0.256*\"silence\" + -0.189*\"fear\" + 0.185*\"year\" + -0.156*\"social\" + 0.151*\"person\" + -0.149*\"said\" + 0.147*\"woman\" + 0.139*\"magazine\"')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Instantiate LDA model\n",
    "LSI = gensim.models.LsiModel\n",
    "\n",
    "df = filtered_data[0: 100000]\n",
    "# df = filtered_data\n",
    "\n",
    "\n",
    "# create dictionary'\n",
    "dictionary = corpora.Dictionary(df['clean_tokens'])\n",
    "\n",
    "# Total number of non-zeroes in the BOW matrix (sum of the number of unique words per document over the entire corpus).\n",
    "# print(dictionary.token2id)\n",
    "\n",
    "\n",
    "# # Create document term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in df['clean_tokens'] ]\n",
    "# print(doc_term_matrix)\n",
    "\n",
    "\n",
    "number_of_topics = 10\n",
    "words = 10\n",
    "LSIModel = LSI(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)\n",
    "print(LSIModel.print_topics(num_topics=number_of_topics, num_words=words))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "366263fc164d38cd9f56da4745dc259c2f24984323fb0b80911bc6d2cc01386b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
